{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto Multimodal Hello World - Iris\n",
    "\n",
    "Este projeto é uma demonstração prática de como implementar um algoritmo multimodal para classificação de imagens de flores. O algoritmo combina informações de duas fontes diferentes: imagens e dados tabulares.\n",
    "\n",
    "Primeiro, carregamos um modelo pré-treinado ResNet50V2 e o usamos para extrair características das imagens de flores. As camadas superiores do modelo são congeladas, o que significa que apenas usamos o modelo para extração de características e não o treinamos com nossos dados.\n",
    "\n",
    "Em seguida, carregamos um conjunto de dados tabulares que contém características adicionais das flores. Usamos um codificador de rótulos para transformar as categorias de flores em números.\n",
    "\n",
    "Os dados de imagem e tabulares são então divididos em conjuntos de treinamento e teste. O modelo de imagem é usado para extrair características das imagens de treinamento, que são então achatadas e concatenadas com os dados tabulares.\n",
    "\n",
    "Finalmente, treinamos um modelo de aprendizado de máquina nos dados combinados e usamos o modelo treinado para fazer previsões no conjunto de teste.\n",
    "\n",
    "Este projeto demonstra como combinar diferentes tipos de dados em um único modelo de aprendizado de máquina e pode ser uma base útil para projetos futuros que envolvem dados multimodais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 19:26:09.532684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 19:26:11.614485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar o conjunto de dados iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar e preparar os dados de imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Carregar imagens de íris\n",
    "# Inicializa um gerador de dados de imagem que irá escalar os valores dos pixels para o intervalo [0, 1]\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Configura o gerador para carregar imagens do diretório especificado, redimensioná-las para 224x224 pixels, carregar em lotes de 32 imagens e usar codificação one-hot para os rótulos de classe\n",
    "generator = train_datagen.flow_from_directory(\n",
    "    directory=\"sample/iris_raw_images\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Extrai os dados do gerador\n",
    "# Inicializa uma lista para armazenar os dados e um índice de lote\n",
    "data_list = []\n",
    "batch_index = 0\n",
    "\n",
    "# Enquanto o índice do lote for menor ou igual ao índice do lote do gerador, obtém o próximo lote de dados e adiciona as imagens à lista de dados\n",
    "while batch_index <= generator.batch_index:\n",
    "    data = next(generator)\n",
    "    data_list.append(data[0])  # data[0] contém as imagens\n",
    "    batch_index += 1\n",
    "\n",
    "# Concatene os dados em um array numpy\n",
    "# Concatena todos os lotes de imagens em um único array numpy\n",
    "X_img = np.concatenate(data_list, axis=0)\n",
    "\n",
    "# Obtem os rótulos de classe das imagens do gerador\n",
    "y_img = generator.classes\n",
    "\n",
    "# Separar dados em treino e teste\n",
    "# Divide os dados de imagem e os rótulos de classe em conjuntos de treinamento e teste\n",
    "# 80% dos dados serão usados para treinamento e 20% para teste\n",
    "# 'random_state=42' garante que a divisão seja reproduzível\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(X_img, y_img, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dataframe com características das flores\n",
    "# Lê o arquivo CSV que contém as características das imagens das flores e armazena em um dataframe\n",
    "df = pd.read_csv(\"sample/iris_raw_images/iris_images.csv\")\n",
    "\n",
    "# Crie um label encoder\n",
    "# Inicializa um objeto LabelEncoder, que pode transformar rótulos categóricos em números\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajuste o label encoder e transforme as categorias em números\n",
    "# Ajusta o LabelEncoder aos rótulos de classe e transforma os rótulos de classe em números\n",
    "df['classe'] = le.fit_transform(df['classe'])\n",
    "\n",
    "# Separar dados em treino e teste\n",
    "# Cria dataframes separados para as características (X_df) e os rótulos de classe (y_df)\n",
    "X_df = df.drop([\"classe\", \"nome_arquivo\"], axis=1)\n",
    "y_df = df[\"classe\"]\n",
    "\n",
    "# Divide os dados em conjuntos de treinamento e teste\n",
    "# 80% dos dados serão usados para treinamento e 20% para teste\n",
    "# 'random_state=42' garante que a divisão seja reproduzível\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 19:26:15.090587: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-25 19:26:15.091574: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo ResNet50V2\n",
    "# Inicializa o modelo ResNet50V2 com pesos pré-treinados no ImageNet\n",
    "# 'include_top=False' significa que a última camada (top) do modelo, que é responsável pela classificação, não será incluída\n",
    "# 'input_shape=(224, 224, 3)' define o formato da entrada para imagens de 224x224 pixels com 3 canais de cor (RGB)\n",
    "base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extrair características das imagens\n",
    "# Define todas as camadas do modelo base como não treináveis, ou seja, seus pesos não serão atualizados durante o treinamento\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Cria um novo modelo que tem a mesma entrada que o modelo base e a saída do modelo base como sua saída\n",
    "# Isso efetivamente cria um modelo que pode ser usado para extrair as características das imagens usando o modelo ResNet50V2\n",
    "model_img = Model(inputs=base_model.input, outputs=base_model.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo de imagem para extrair características das imagens de treinamento\n",
    "# O método 'predict' do modelo é usado para processar as imagens de treinamento através do modelo e obter as características extraídas\n",
    "features_img = model_img.predict(X_train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deive/repos/multi_modal_machine_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Criar modelo para características do dataframe\n",
    "# Inicializa um modelo sequencial\n",
    "model_df = Sequential() \n",
    "\n",
    "# Adiciona a primeira camada densa com 64 neurônios e função de ativação ReLU\n",
    "model_df.add(Dense(64, activation='relu', input_shape=X_train_df.shape[1:]))\n",
    "\n",
    "# Adiciona uma camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_df.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a segunda camada densa com 32 neurônios e função de ativação ReLU\n",
    "model_df.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Adiciona outra camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_df.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a camada de saída com um número de neurônios igual ao número de classes únicas em y_train_df e função de ativação softmax\n",
    "model_df.add(Dense(y_train_df.nunique(), activation='softmax'))\n",
    "\n",
    "# Compila modelo\n",
    "# Define a função de perda como entropia cruzada categórica esparsa, o otimizador como Adam e a métrica de avaliação como acurácia\n",
    "model_df.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.5571 - loss: 112.1190 - val_accuracy: 0.5588 - val_loss: 28.2549\n",
      "Epoch 2/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4893 - loss: 59.9046 - val_accuracy: 0.6029 - val_loss: 17.1606\n",
      "Epoch 3/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4371 - loss: 39.8852 - val_accuracy: 0.6176 - val_loss: 15.4625\n",
      "Epoch 4/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5174 - loss: 28.6415 - val_accuracy: 0.6324 - val_loss: 14.9826\n",
      "Epoch 5/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4952 - loss: 25.8948 - val_accuracy: 0.6176 - val_loss: 11.2177\n",
      "Epoch 6/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5014 - loss: 26.8512 - val_accuracy: 0.6324 - val_loss: 10.1235\n",
      "Epoch 7/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4569 - loss: 26.4706 - val_accuracy: 0.6471 - val_loss: 10.8440\n",
      "Epoch 8/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.4616 - loss: 66.5756 - val_accuracy: 0.5882 - val_loss: 9.3810\n",
      "Epoch 9/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.4579 - loss: 19.0417 - val_accuracy: 0.5735 - val_loss: 8.0898\n",
      "Epoch 10/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - accuracy: 0.4774 - loss: 15.1822 - val_accuracy: 0.5882 - val_loss: 6.6232\n",
      "Epoch 11/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.4411 - loss: 26.8568 - val_accuracy: 0.6176 - val_loss: 7.8397\n",
      "Epoch 12/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - accuracy: 0.4890 - loss: 25.1808 - val_accuracy: 0.5735 - val_loss: 3.6791\n",
      "Epoch 13/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.4189 - loss: 28.4706 - val_accuracy: 0.4265 - val_loss: 4.5194\n",
      "Epoch 14/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.4186 - loss: 14.1467 - val_accuracy: 0.5588 - val_loss: 4.2744\n",
      "Epoch 15/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4604 - loss: 24.2543 - val_accuracy: 0.5735 - val_loss: 3.7912\n",
      "Epoch 16/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4294 - loss: 18.7698 - val_accuracy: 0.6176 - val_loss: 3.9998\n"
     ]
    }
   ],
   "source": [
    "# Treina modelo\n",
    "# O método 'fit' é usado para treinar o modelo 'model_df'\n",
    "# 'X_train_df' e 'y_train_df' são os dados de treinamento e os rótulos de treinamento, respectivamente\n",
    "# 'epochs=16' significa que o conjunto de treinamento completo será passado pelo modelo 16 vezes\n",
    "# 'batch_size=32' define o número de amostras a serem propagadas pela rede de uma vez\n",
    "# 'validation_split=0.2' reserva 20% dos dados de treinamento para validação\n",
    "# O histórico de treinamento é armazenado no objeto 'history_df'\n",
    "history_df = model_df.fit(X_train_df, y_train_df, epochs=16, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina características (fusion)\n",
    "# Redimensiona o array 'features_img' para ter 336 linhas e um número de colunas que será calculado automaticamente (-1)\n",
    "features_img_flattened = features_img.reshape(336, -1)\n",
    "\n",
    "# Concatena o array 'features_img_flattened' e o dataframe 'X_train_df' ao longo do eixo 1 (colunas)\n",
    "# O resultado é um novo array 'features' que contém as colunas de ambos 'features_img_flattened' e 'X_train_df'\n",
    "features = np.concatenate([features_img_flattened, X_train_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deive/repos/multi_modal_machine_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - accuracy: 0.5568 - loss: 9.3776 - val_accuracy: 0.5000 - val_loss: 3.7472\n",
      "Epoch 2/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.5390 - loss: 5.1778 - val_accuracy: 0.5882 - val_loss: 3.3714\n",
      "Epoch 3/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.6170 - loss: 4.6311 - val_accuracy: 0.4706 - val_loss: 2.5597\n",
      "Epoch 4/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.7031 - loss: 2.9114 - val_accuracy: 0.5882 - val_loss: 2.8974\n",
      "Epoch 5/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 171ms/step - accuracy: 0.7805 - loss: 1.4437 - val_accuracy: 0.4853 - val_loss: 2.6813\n",
      "Epoch 6/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 190ms/step - accuracy: 0.7770 - loss: 1.3047 - val_accuracy: 0.6471 - val_loss: 2.6569\n",
      "Epoch 7/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - accuracy: 0.8260 - loss: 2.1822 - val_accuracy: 0.6176 - val_loss: 2.5201\n",
      "Epoch 8/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - accuracy: 0.8238 - loss: 0.7894 - val_accuracy: 0.6029 - val_loss: 2.3966\n",
      "Epoch 9/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 165ms/step - accuracy: 0.8195 - loss: 0.8331 - val_accuracy: 0.5735 - val_loss: 2.4444\n",
      "Epoch 10/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.8494 - loss: 0.7209 - val_accuracy: 0.5441 - val_loss: 2.3160\n",
      "Epoch 11/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.8150 - loss: 1.0645 - val_accuracy: 0.5882 - val_loss: 2.2706\n",
      "Epoch 12/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.8474 - loss: 0.9976 - val_accuracy: 0.6324 - val_loss: 2.4445\n",
      "Epoch 13/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.8826 - loss: 0.4342 - val_accuracy: 0.6029 - val_loss: 2.5033\n",
      "Epoch 14/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.8995 - loss: 0.6012 - val_accuracy: 0.5882 - val_loss: 2.7874\n",
      "Epoch 15/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - accuracy: 0.8833 - loss: 0.5409 - val_accuracy: 0.6176 - val_loss: 3.0592\n",
      "Epoch 16/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.8885 - loss: 0.3824 - val_accuracy: 0.5735 - val_loss: 2.1881\n"
     ]
    }
   ],
   "source": [
    "# Cria modelo final\n",
    "model_final = Sequential()  # Inicializa um modelo sequencial\n",
    "\n",
    "# Adiciona a primeira camada densa com 128 neurônios e função de ativação ReLU\n",
    "model_final.add(Dense(128, activation='relu', input_shape=features.shape[1:]))\n",
    "\n",
    "# Adiciona uma camada de dropout para evitar overfitting, descartando 30% dos neurônios\n",
    "model_final.add(Dropout(0.3))\n",
    "\n",
    "# Adiciona a segunda camada densa com 64 neurônios e função de ativação ReLU\n",
    "model_final.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adiciona outra camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_final.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a camada de saída com 3 neurônios (para 3 classes) e função de ativação softmax\n",
    "model_final.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compila modelo\n",
    "# Define a função de perda como entropia cruzada categórica esparsa, o otimizador como Adam e a métrica de avaliação como acurácia\n",
    "model_final.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treina modelo final\n",
    "# Treina o modelo usando os dados de treinamento, com 16 épocas, tamanho de lote de 32 e 20% dos dados usados para validação\n",
    "history_final = model_final.fit(features, y_train_img, epochs=16, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo de imagem para extrair características das imagens de teste\n",
    "# O método 'predict' do modelo é usado para processar as imagens de teste através do modelo e obter as características extraídas\n",
    "features_img_test = model_img.predict(X_test_img)\n",
    "\n",
    "# Redimensiona as características da imagem para terem uma dimensão de (número de imagens, -1)\n",
    "# O -1 significa que o tamanho da segunda dimensão é calculado automaticamente com base no tamanho da matriz original\n",
    "features_img_flattened_test = features_img_test.reshape(len(y_test_img), -1)\n",
    "\n",
    "# Concatena as características da imagem e as características do dataframe para criar um conjunto de características completo para teste\n",
    "# 'axis=1' significa que a concatenação é feita ao longo do eixo das colunas (ou seja, as características do dataframe são adicionadas como colunas adicionais às características da imagem)\n",
    "features_test = np.concatenate([features_img_flattened_test, X_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo final para fazer previsões no conjunto de teste\n",
    "# O método 'predict' do modelo é usado para processar as características de teste através do modelo e obter as previsões\n",
    "# As previsões são armazenadas na variável 'prevision'\n",
    "prevision = model_final.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte probabilidade para labels da classe\n",
    "predicted_classes = np.argmax(prevision, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 56.47%\n"
     ]
    }
   ],
   "source": [
    "# Calcula acurácia\n",
    "accuracy = accuracy_score(y_test_img, predicted_classes)\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
