{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto Multimodal Hello World - Iris\n",
    "\n",
    "Este projeto é uma demonstração prática de como implementar um algoritmo multimodal para classificação de imagens de flores. O algoritmo combina informações de duas fontes diferentes: imagens e dados tabulares.\n",
    "\n",
    "Primeiro, carregamos um modelo pré-treinado ResNet50V2 e o usamos para extrair características das imagens de flores. As camadas superiores do modelo são congeladas, o que significa que apenas usamos o modelo para extração de características e não o treinamos com nossos dados.\n",
    "\n",
    "Em seguida, carregamos um conjunto de dados tabulares que contém características adicionais das flores. Usamos um codificador de rótulos para transformar as categorias de flores em números.\n",
    "\n",
    "Os dados de imagem e tabulares são então divididos em conjuntos de treinamento e teste. O modelo de imagem é usado para extrair características das imagens de treinamento, que são então achatadas e concatenadas com os dados tabulares.\n",
    "\n",
    "Finalmente, treinamos um modelo de aprendizado de máquina nos dados combinados e usamos o modelo treinado para fazer previsões no conjunto de teste.\n",
    "\n",
    "Este projeto demonstra como combinar diferentes tipos de dados em um único modelo de aprendizado de máquina e pode ser uma base útil para projetos futuros que envolvem dados multimodais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar o conjunto de dados iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar e preparar os dados de imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Carregar imagens de íris\n",
    "# Inicializa um gerador de dados de imagem que irá escalar os valores dos pixels para o intervalo [0, 1]\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Configura o gerador para carregar imagens do diretório especificado, redimensioná-las para 224x224 pixels, carregar em lotes de 32 imagens e usar codificação one-hot para os rótulos de classe\n",
    "generator = train_datagen.flow_from_directory(\n",
    "    directory=\"sample/iris_raw_images\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Extrai os dados do gerador\n",
    "# Inicializa uma lista para armazenar os dados e um índice de lote\n",
    "data_list = []\n",
    "batch_index = 0\n",
    "\n",
    "# Enquanto o índice do lote for menor ou igual ao índice do lote do gerador, obtém o próximo lote de dados e adiciona as imagens à lista de dados\n",
    "while batch_index <= generator.batch_index:\n",
    "    data = next(generator)\n",
    "    data_list.append(data[0])  # data[0] contém as imagens\n",
    "    batch_index += 1\n",
    "\n",
    "# Concatene os dados em um array numpy\n",
    "# Concatena todos os lotes de imagens em um único array numpy\n",
    "X_img = np.concatenate(data_list, axis=0)\n",
    "\n",
    "# Obtem os rótulos de classe das imagens do gerador\n",
    "y_img = generator.classes\n",
    "\n",
    "# Separar dados em treino e teste\n",
    "# Divide os dados de imagem e os rótulos de classe em conjuntos de treinamento e teste\n",
    "# 80% dos dados serão usados para treinamento e 20% para teste\n",
    "# 'random_state=42' garante que a divisão seja reproduzível\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(X_img, y_img, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dataframe com características das flores\n",
    "# Lê o arquivo CSV que contém as características das imagens das flores e armazena em um dataframe\n",
    "df = pd.read_csv(\"sample/iris_raw_images/iris_images.csv\")\n",
    "\n",
    "# Crie um label encoder\n",
    "# Inicializa um objeto LabelEncoder, que pode transformar rótulos categóricos em números\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajuste o label encoder e transforme as categorias em números\n",
    "# Ajusta o LabelEncoder aos rótulos de classe e transforma os rótulos de classe em números\n",
    "df['classe'] = le.fit_transform(df['classe'])\n",
    "\n",
    "# Separar dados em treino e teste\n",
    "# Cria dataframes separados para as características (X_df) e os rótulos de classe (y_df)\n",
    "X_df = df.drop([\"classe\", \"nome_arquivo\"], axis=1)\n",
    "y_df = df[\"classe\"]\n",
    "\n",
    "# Divide os dados em conjuntos de treinamento e teste\n",
    "# 80% dos dados serão usados para treinamento e 20% para teste\n",
    "# 'random_state=42' garante que a divisão seja reproduzível\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo ResNet50V2\n",
    "# Inicializa o modelo ResNet50V2 com pesos pré-treinados no ImageNet\n",
    "# 'include_top=False' significa que a última camada (top) do modelo, que é responsável pela classificação, não será incluída\n",
    "# 'input_shape=(224, 224, 3)' define o formato da entrada para imagens de 224x224 pixels com 3 canais de cor (RGB)\n",
    "base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extrair características das imagens\n",
    "# Define todas as camadas do modelo base como não treináveis, ou seja, seus pesos não serão atualizados durante o treinamento\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Cria um novo modelo que tem a mesma entrada que o modelo base e a saída do modelo base como sua saída\n",
    "# Isso efetivamente cria um modelo que pode ser usado para extrair as características das imagens usando o modelo ResNet50V2\n",
    "model_img = Model(inputs=base_model.input, outputs=base_model.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f4a34e960c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo de imagem para extrair características das imagens de treinamento\n",
    "# O método 'predict' do modelo é usado para processar as imagens de treinamento através do modelo e obter as características extraídas\n",
    "features_img = model_img.predict(X_train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deive/repos/multi_modal_machine_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Criar modelo para características do dataframe\n",
    "# Inicializa um modelo sequencial\n",
    "model_df = Sequential() \n",
    "\n",
    "# Adiciona a primeira camada densa com 64 neurônios e função de ativação ReLU\n",
    "model_df.add(Dense(64, activation='relu', input_shape=X_train_df.shape[1:]))\n",
    "\n",
    "# Adiciona uma camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_df.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a segunda camada densa com 32 neurônios e função de ativação ReLU\n",
    "model_df.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Adiciona outra camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_df.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a camada de saída com um número de neurônios igual ao número de classes únicas em y_train_df e função de ativação softmax\n",
    "model_df.add(Dense(y_train_df.nunique(), activation='softmax'))\n",
    "\n",
    "# Compila modelo\n",
    "# Define a função de perda como entropia cruzada categórica esparsa, o otimizador como Adam e a métrica de avaliação como acurácia\n",
    "model_df.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.4328 - loss: 79.5324 - val_accuracy: 0.6324 - val_loss: 36.7390\n",
      "Epoch 2/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5337 - loss: 36.0796 - val_accuracy: 0.6471 - val_loss: 30.4171\n",
      "Epoch 3/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6190 - loss: 62.1408 - val_accuracy: 0.6176 - val_loss: 27.4177\n",
      "Epoch 4/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5415 - loss: 88.5760 - val_accuracy: 0.5147 - val_loss: 20.6698\n",
      "Epoch 5/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4895 - loss: 79.9725 - val_accuracy: 0.4118 - val_loss: 16.4218\n",
      "Epoch 6/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4532 - loss: 22.8070 - val_accuracy: 0.4118 - val_loss: 12.6826\n",
      "Epoch 7/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3946 - loss: 22.5608 - val_accuracy: 0.5147 - val_loss: 9.8118\n",
      "Epoch 8/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4676 - loss: 17.3559 - val_accuracy: 0.5588 - val_loss: 8.6335\n",
      "Epoch 9/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5434 - loss: 17.1888 - val_accuracy: 0.5588 - val_loss: 7.1594\n",
      "Epoch 10/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5079 - loss: 39.4260 - val_accuracy: 0.5882 - val_loss: 7.7847\n",
      "Epoch 11/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5431 - loss: 57.1536 - val_accuracy: 0.5735 - val_loss: 7.1639\n",
      "Epoch 12/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5206 - loss: 12.9608 - val_accuracy: 0.4265 - val_loss: 5.6367\n",
      "Epoch 13/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5124 - loss: 21.3913 - val_accuracy: 0.4265 - val_loss: 4.1968\n",
      "Epoch 14/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4662 - loss: 17.8139 - val_accuracy: 0.5882 - val_loss: 7.0843\n",
      "Epoch 15/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6003 - loss: 13.8023 - val_accuracy: 0.6029 - val_loss: 8.1591\n",
      "Epoch 16/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4789 - loss: 37.7244 - val_accuracy: 0.5735 - val_loss: 7.8661\n"
     ]
    }
   ],
   "source": [
    "# Treina modelo\n",
    "# O método 'fit' é usado para treinar o modelo 'model_df'\n",
    "# 'X_train_df' e 'y_train_df' são os dados de treinamento e os rótulos de treinamento, respectivamente\n",
    "# 'epochs=16' significa que o conjunto de treinamento completo será passado pelo modelo 16 vezes\n",
    "# 'batch_size=32' define o número de amostras a serem propagadas pela rede de uma vez\n",
    "# 'validation_split=0.2' reserva 20% dos dados de treinamento para validação\n",
    "# O histórico de treinamento é armazenado no objeto 'history_df'\n",
    "history_df = model_df.fit(X_train_df, y_train_df, epochs=16, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina características (fusion)\n",
    "# Redimensiona o array 'features_img' para ter 336 linhas e um número de colunas que será calculado automaticamente (-1)\n",
    "features_img_flattened = features_img.reshape(336, -1)\n",
    "\n",
    "# Concatena o array 'features_img_flattened' e o dataframe 'X_train_df' ao longo do eixo 1 (colunas)\n",
    "# O resultado é um novo array 'features' que contém as colunas de ambos 'features_img_flattened' e 'X_train_df'\n",
    "features = np.concatenate([features_img_flattened, X_train_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deive/repos/multi_modal_machine_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 236ms/step - accuracy: 0.5180 - loss: 7.2850 - val_accuracy: 0.6324 - val_loss: 5.9400\n",
      "Epoch 2/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 196ms/step - accuracy: 0.5640 - loss: 6.6583 - val_accuracy: 0.6176 - val_loss: 3.6482\n",
      "Epoch 3/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.7049 - loss: 4.7061 - val_accuracy: 0.5735 - val_loss: 3.4911\n",
      "Epoch 4/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 191ms/step - accuracy: 0.7032 - loss: 2.3672 - val_accuracy: 0.4706 - val_loss: 2.5658\n",
      "Epoch 5/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 207ms/step - accuracy: 0.7273 - loss: 1.3755 - val_accuracy: 0.5588 - val_loss: 2.1156\n",
      "Epoch 6/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - accuracy: 0.7435 - loss: 2.5149 - val_accuracy: 0.6029 - val_loss: 2.1041\n",
      "Epoch 7/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - accuracy: 0.7923 - loss: 1.2361 - val_accuracy: 0.5882 - val_loss: 2.6270\n",
      "Epoch 8/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - accuracy: 0.8587 - loss: 0.8160 - val_accuracy: 0.5294 - val_loss: 2.8161\n",
      "Epoch 9/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 196ms/step - accuracy: 0.8359 - loss: 1.1274 - val_accuracy: 0.5735 - val_loss: 2.9040\n",
      "Epoch 10/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step - accuracy: 0.8743 - loss: 0.6515 - val_accuracy: 0.6176 - val_loss: 3.8710\n",
      "Epoch 11/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.8704 - loss: 0.6219 - val_accuracy: 0.6176 - val_loss: 3.0683\n",
      "Epoch 12/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - accuracy: 0.8851 - loss: 0.4660 - val_accuracy: 0.6029 - val_loss: 2.2939\n",
      "Epoch 13/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - accuracy: 0.8291 - loss: 0.7850 - val_accuracy: 0.6029 - val_loss: 2.5369\n",
      "Epoch 14/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - accuracy: 0.8792 - loss: 1.0365 - val_accuracy: 0.6029 - val_loss: 2.4555\n",
      "Epoch 15/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.8935 - loss: 0.5390 - val_accuracy: 0.6029 - val_loss: 3.0600\n",
      "Epoch 16/16\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.9002 - loss: 0.8288 - val_accuracy: 0.6029 - val_loss: 3.5391\n"
     ]
    }
   ],
   "source": [
    "# Cria modelo final\n",
    "model_final = Sequential()  # Inicializa um modelo sequencial\n",
    "\n",
    "# Adiciona a primeira camada densa com 128 neurônios e função de ativação ReLU\n",
    "model_final.add(Dense(128, activation='relu', input_shape=features.shape[1:]))\n",
    "\n",
    "# Adiciona uma camada de dropout para evitar overfitting, descartando 30% dos neurônios\n",
    "model_final.add(Dropout(0.3))\n",
    "\n",
    "# Adiciona a segunda camada densa com 64 neurônios e função de ativação ReLU\n",
    "model_final.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adiciona outra camada de dropout para evitar overfitting, descartando 20% dos neurônios\n",
    "model_final.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona a camada de saída com 3 neurônios (para 3 classes) e função de ativação softmax\n",
    "model_final.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compila modelo\n",
    "# Define a função de perda como entropia cruzada categórica esparsa, o otimizador como Adam e a métrica de avaliação como acurácia\n",
    "model_final.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treina modelo final\n",
    "# Treina o modelo usando os dados de treinamento, com 16 épocas, tamanho de lote de 32 e 20% dos dados usados para validação\n",
    "history_final = model_final.fit(features, y_train_img, epochs=16, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo de imagem para extrair características das imagens de teste\n",
    "# O método 'predict' do modelo é usado para processar as imagens de teste através do modelo e obter as características extraídas\n",
    "features_img_test = model_img.predict(X_test_img)\n",
    "\n",
    "# Redimensiona as características da imagem para terem uma dimensão de (número de imagens, -1)\n",
    "# O -1 significa que o tamanho da segunda dimensão é calculado automaticamente com base no tamanho da matriz original\n",
    "features_img_flattened_test = features_img_test.reshape(len(y_test_img), -1)\n",
    "\n",
    "# Concatena as características da imagem e as características do dataframe para criar um conjunto de características completo para teste\n",
    "# 'axis=1' significa que a concatenação é feita ao longo do eixo das colunas (ou seja, as características do dataframe são adicionadas como colunas adicionais às características da imagem)\n",
    "features_test = np.concatenate([features_img_flattened_test, X_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "# Usa o modelo final para fazer previsões no conjunto de teste\n",
    "# O método 'predict' do modelo é usado para processar as características de teste através do modelo e obter as previsões\n",
    "# As previsões são armazenadas na variável 'prevision'\n",
    "prevision = model_final.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte probabilidade para labels da classe\n",
    "predicted_classes = np.argmax(prevision, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 52.94%\n"
     ]
    }
   ],
   "source": [
    "# Calcula acurácia\n",
    "accuracy = accuracy_score(y_test_img, predicted_classes)\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
